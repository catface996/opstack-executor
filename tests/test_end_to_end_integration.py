"""
端到端集成测试

测试从团队创建到结果输出的完整工作流程，验证所有需求的实现。
"""

import pytest
import asyncio
import json
import os
import tempfile
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime
from typing import Dict, Any, List

from hierarchical_agents.data_models import (
    HierarchicalTeam, SubTeam, AgentConfig, SupervisorConfig, 
    LLMConfig, GlobalConfig, ExecutionEvent, ExecutionStatus,
    StandardizedOutput, ExecutionSummary, TeamResult
)
from hierarchical_agents.hierarchical_manager import HierarchicalManager
from hierarchical_agents.team_builder import TeamBuilder
from hierarchical_agents.execution_engine import ExecutionEngine
from hierarchical_agents.hierarchical_executor import HierarchicalExecutor
from hierarchical_agents.output_formatter import OutputFormatter
from hierarchical_agents.state_manager import StateManager
from hierarchical_agents.event_manager import EventManager
from hierarchical_agents.env_key_manager import EnvironmentKeyManager
from hierarchical_agents.agents import SupervisorAgent, WorkerAgent
from hierarchical_agents.error_handler import ErrorHandler
from hierarchical_agents.logging_monitor import LoggingMonitor


class TestEndToEndIntegration:
    """端到端集成测试类"""
    
    @pytest.fixture
    def sample_team_config(self):
        """创建示例团队配置"""
        return {
            "team_name": "research_analysis_team",
            "description": "研究分析团队",
            "top_supervisor_config": {
                "llm_config": {
                    "provider": "openai",
                    "model": "gpt-4o",
                    "temperature": 0.3,
                    "max_tokens": 1000
                },
                "system_prompt": "你是顶级监督者，负责协调整个分层团队。",
                "user_prompt": "请协调团队执行研究分析任务。",
                "max_iterations": 10
            },
            "sub_teams": [
                {
                    "id": "research_team",
                    "name": "研究团队", 
                    "description": "负责信息收集和研究",
                    "supervisor_config": {
                        "llm_config": {
                            "provider": "openai",
                            "model": "gpt-4o",
                            "temperature": 0.3
                        },
                        "system_prompt": "你是研究团队监督者。",
                        "user_prompt": "请协调研究团队执行任务。",
                        "max_iterations": 8
                    },
                    "agent_configs": [
                        {
                            "agent_id": "search_agent",
                            "agent_name": "搜索专家",
                            "llm_config": {
                                "provider": "openai",
                                "model": "gpt-4o",
                                "temperature": 0.3
                            },
                            "system_prompt": "你是搜索专家。",
                            "user_prompt": "请搜索相关信息。",
                            "tools": ["search_tool"],
                            "max_iterations": 5
                        }
                    ]
                }
            ],
            "dependencies": {},
            "global_config": {
                "max_execution_time": 3600,
                "enable_streaming": True,
                "output_format": "detailed"
            }
        }
    
    @pytest.fixture
    def mock_key_manager(self):
        """Mock密钥管理器"""
        key_manager = Mock()
        
        # Mock LLM客户端
        mock_llm = Mock()
        mock_llm.invoke = Mock(return_value=Mock(content="研究团队"))
        mock_llm.with_structured_output = Mock(return_value=mock_llm)
        
        key_manager.get_llm_client = Mock(return_value=mock_llm)
        key_manager.validate_key_format = Mock(return_value=True)
        key_manager.get_api_key = Mock(return_value="sk-test-api-key-123")
        return key_manager
    
    @pytest.fixture
    def mock_state_manager(self):
        """Mock状态管理器"""
        state_manager = Mock()
        state_manager.update_execution_state = AsyncMock()
        state_manager.get_execution_state = AsyncMock(return_value={
            "status": "completed",
            "started_at": datetime.now().isoformat(),
            "completed_at": datetime.now().isoformat()
        })
        return state_manager
    
    @pytest.fixture
    def mock_event_manager(self):
        """Mock事件管理器"""
        event_manager = Mock()
        event_manager.emit_event = AsyncMock()
        
        # Mock事件流
        async def mock_stream():
            events = [
                {
                    "timestamp": datetime.now().isoformat(),
                    "event_type": "execution_started",
                    "source_type": "system",
                    "execution_id": "test_exec_123"
                },
                {
                    "timestamp": datetime.now().isoformat(),
                    "event_type": "execution_completed",
                    "source_type": "system", 
                    "execution_id": "test_exec_123"
                }
            ]
            for event in events:
                yield event
        
        event_manager.get_event_stream = Mock(return_value=mock_stream())
        return event_manager


class TestRequirement1HierarchicalTeamBuilding(TestEndToEndIntegration):
    """需求1验证：测试分层团队构建的完整功能"""
    
    @pytest.mark.asyncio
    async def test_complete_team_building_workflow(self, sample_team_config, mock_key_manager):
        """测试完整的团队构建工作流程"""
        with patch('hierarchical_agents.hierarchical_manager.TeamBuilder') as mock_team_builder_class:
            # Mock团队构建器
            mock_team_builder = Mock()
            mock_team_builder_class.return_value = mock_team_builder
            
            # Mock构建的团队
            mock_team = Mock()
            mock_team.team_name = "research_analysis_team"
            mock_team.sub_teams = [Mock(name="研究团队")]
            mock_team.sub_teams[0].name = "研究团队"
            
            # Mock顶级监督者
            mock_supervisor = Mock()
            mock_team.top_supervisor = mock_supervisor
            
            # Mock子团队结构
            mock_research_team = Mock()
            mock_research_team.supervisor = Mock()
            mock_agent = Mock()
            mock_agent.config.agent_name = "搜索专家"
            mock_research_team.agents = [mock_agent]
            mock_team.teams = {"research_team": mock_research_team}
            
            mock_team_builder.validate_team_configuration.return_value = (True, [])
            mock_team_builder.build_hierarchical_team.return_value = mock_team
            
            # 1. 创建分层管理器
            manager = HierarchicalManager(key_manager=mock_key_manager)
            
            # 2. 构建分层团队
            team = manager.build_hierarchy(sample_team_config)
            
            # 验证团队结构
            assert team is not None
            assert team.team_name == "research_analysis_team"
            assert len(team.sub_teams) == 1
            assert team.sub_teams[0].name == "研究团队"
            
            # 验证顶级监督者
            assert team.top_supervisor is not None
            
            # 验证子团队结构
            research_team = team.teams["research_team"]
            assert research_team is not None
            assert research_team.supervisor is not None
            assert len(research_team.agents) == 1
            assert research_team.agents[0].config.agent_name == "搜索专家"
    
    @pytest.mark.asyncio
    async def test_dependency_graph_creation(self, mock_key_manager):
        """测试依赖关系图的创建"""
        # 创建有依赖关系的团队配置
        config_with_deps = {
            "team_name": "complex_team",
            "description": "复杂团队",
            "top_supervisor_config": {
                "llm_config": {"provider": "openai", "model": "gpt-4o"},
                "system_prompt": "顶级监督者",
                "user_prompt": "协调执行",
                "max_iterations": 10
            },
            "sub_teams": [
                {
                    "id": "team_a",
                    "name": "团队A",
                    "description": "第一个团队",
                    "supervisor_config": {
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "团队A监督者",
                        "user_prompt": "执行任务A",
                        "max_iterations": 5
                    },
                    "agent_configs": [{
                        "agent_id": "agent_a1",
                        "agent_name": "智能体A1",
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "智能体A1",
                        "user_prompt": "执行子任务",
                        "tools": [],
                        "max_iterations": 3
                    }]
                },
                {
                    "id": "team_b", 
                    "name": "团队B",
                    "description": "第二个团队",
                    "supervisor_config": {
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "团队B监督者",
                        "user_prompt": "执行任务B",
                        "max_iterations": 5
                    },
                    "agent_configs": [{
                        "agent_id": "agent_b1",
                        "agent_name": "智能体B1", 
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "智能体B1",
                        "user_prompt": "执行子任务",
                        "tools": [],
                        "max_iterations": 3
                    }]
                }
            ],
            "dependencies": {"team_b": ["team_a"]},  # team_b依赖team_a
            "global_config": {
                "max_execution_time": 3600,
                "enable_streaming": True,
                "output_format": "detailed"
            }
        }
        
        manager = HierarchicalManager(key_manager=mock_key_manager)
        team = manager.build_hierarchy(config_with_deps)
        
        # 验证依赖关系图
        assert team.dependency_graph is not None
        assert team.execution_order is not None
        
        # 验证执行顺序：team_a应该在team_b之前
        team_a_index = team.execution_order.index("team_a")
        team_b_index = team.execution_order.index("team_b")
        assert team_a_index < team_b_index
    
    @pytest.mark.asyncio
    async def test_agent_configuration_flexibility(self, mock_key_manager):
        """测试智能体配置的灵活性"""
        # 创建多样化的智能体配置
        diverse_config = {
            "team_name": "diverse_team",
            "description": "多样化团队",
            "top_supervisor_config": {
                "llm_config": {"provider": "openai", "model": "gpt-4o"},
                "system_prompt": "顶级监督者",
                "user_prompt": "协调执行",
                "max_iterations": 10
            },
            "sub_teams": [{
                "id": "diverse_team",
                "name": "多样化团队",
                "description": "包含不同类型智能体的团队",
                "supervisor_config": {
                    "llm_config": {"provider": "openai", "model": "gpt-4o"},
                    "system_prompt": "多样化团队监督者",
                    "user_prompt": "协调多样化任务",
                    "max_iterations": 8
                },
                "agent_configs": [
                    {
                        "agent_id": "search_specialist",
                        "agent_name": "搜索专家",
                        "llm_config": {
                            "provider": "openai",
                            "model": "gpt-4o",
                            "temperature": 0.3,
                            "max_tokens": 2000
                        },
                        "system_prompt": "专业搜索智能体",
                        "user_prompt": "执行搜索任务",
                        "tools": ["search_tool", "web_scraper"],
                        "max_iterations": 5
                    },
                    {
                        "agent_id": "analysis_expert",
                        "agent_name": "分析专家",
                        "llm_config": {
                            "provider": "openai",
                            "model": "gpt-4o",
                            "temperature": 0.5,
                            "max_tokens": 3000
                        },
                        "system_prompt": "数据分析专家",
                        "user_prompt": "执行分析任务",
                        "tools": ["data_processor"],
                        "max_iterations": 3
                    },
                    {
                        "agent_id": "writer_agent",
                        "agent_name": "写作专家",
                        "llm_config": {
                            "provider": "openai",
                            "model": "gpt-4o",
                            "temperature": 0.7,
                            "max_tokens": 4000
                        },
                        "system_prompt": "专业写作智能体",
                        "user_prompt": "执行写作任务",
                        "tools": ["document_writer", "editor"],
                        "max_iterations": 5
                    }
                ]
            }],
            "dependencies": {},
            "global_config": {
                "max_execution_time": 3600,
                "enable_streaming": True,
                "output_format": "detailed"
            }
        }
        
        manager = HierarchicalManager(key_manager=mock_key_manager)
        team = manager.build_hierarchy(diverse_config)
        
        # 验证多样化配置
        diverse_team = team.teams["diverse_team"]
        assert len(diverse_team.agents) == 3
        
        # 验证不同智能体的配置
        agents_by_name = {agent.config.agent_name: agent for agent in diverse_team.agents}
        
        search_agent = agents_by_name["搜索专家"]
        assert search_agent.config.llm_config.temperature == 0.3
        assert "search_tool" in search_agent.config.tools
        
        analysis_agent = agents_by_name["分析专家"]
        assert analysis_agent.config.llm_config.temperature == 0.5
        assert "data_processor" in analysis_agent.config.tools
        
        writer_agent = agents_by_name["写作专家"]
        assert writer_agent.config.llm_config.temperature == 0.7
        assert "document_writer" in writer_agent.config.tools


class TestRequirement2ExecutionAndStreaming(TestEndToEndIntegration):
    """需求2验证：测试执行触发和流式反馈功能"""
    
    @pytest.mark.asyncio
    async def test_execution_trigger_and_workflow(self, sample_team_config, mock_key_manager, 
                                                 mock_state_manager, mock_event_manager):
        """测试执行触发和完整工作流程"""
        # 1. 构建团队
        manager = HierarchicalManager(key_manager=mock_key_manager)
        team = manager.build_hierarchy(sample_team_config)
        
        # 2. 创建执行引擎
        execution_engine = ExecutionEngine(
            state_manager=mock_state_manager,
            event_manager=mock_event_manager
        )
        
        # 3. 创建分层执行器
        executor = HierarchicalExecutor(
            state_manager=mock_state_manager,
            event_manager=mock_event_manager
        )
        
        # 4. 启动执行
        execution_id = "test_exec_123"
        
        # Mock执行结果
        mock_result = {
            "execution_id": execution_id,
            "status": "completed",
            "team_results": {
                "research_team": {
                    "status": "completed",
                    "agents": {
                        "search_agent": {
                            "status": "completed",
                            "output": "搜索完成，找到相关信息"
                        }
                    }
                }
            }
        }
        
        with patch.object(executor, 'execute_hierarchical_team', new_callable=AsyncMock) as mock_execute:
            mock_execute.return_value = mock_result
            
            # 执行团队
            result = await executor.execute_hierarchical_team(team, execution_id)
            
            # 验证执行结果
            assert result is not None
            assert result["execution_id"] == execution_id
            assert result["status"] == "completed"
            assert "research_team" in result["team_results"]
            
            # 验证执行方法被调用
            mock_execute.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_streaming_events_functionality(self, mock_event_manager):
        """测试流式事件功能"""
        # 创建事件流
        events = []
        
        async def collect_events():
            async for event in mock_event_manager.get_event_stream():
                events.append(event)
        
        # 收集事件
        await collect_events()
        
        # 验证事件流
        assert len(events) == 2
        assert events[0].event_type == "execution_started"
        assert events[1].event_type == "execution_completed"
        assert all(event.execution_id == "test_exec_123" for event in events)
    
    @pytest.mark.asyncio
    async def test_real_time_status_updates(self, sample_team_config, mock_key_manager,
                                          mock_state_manager, mock_event_manager):
        """测试实时状态更新"""
        # 构建团队
        with patch('hierarchical_agents.hierarchical_manager.TeamBuilder') as mock_team_builder_class:
            mock_team_builder = Mock()
            mock_team_builder_class.return_value = mock_team_builder
            mock_team_builder.validate_team_configuration.return_value = (True, [])
            mock_team_builder.build_hierarchical_team.return_value = Mock()
            
            manager = HierarchicalManager(key_manager=mock_key_manager)
            team = manager.build_hierarchy(sample_team_config)
        
        # 创建执行器
        executor = HierarchicalExecutor(
            state_manager=mock_state_manager,
            event_manager=mock_event_manager
        )
        
        execution_id = "test_exec_456"
        
        # Mock状态更新序列
        status_updates = [
            {"status": "pending", "progress": 0},
            {"status": "running", "progress": 25},
            {"status": "running", "progress": 50},
            {"status": "running", "progress": 75},
            {"status": "completed", "progress": 100}
        ]
        
        mock_state_manager.get_execution_state.side_effect = status_updates
        
        # 模拟状态查询
        for expected_status in status_updates:
            current_state = await mock_state_manager.get_execution_state(execution_id)
            assert current_state["status"] == expected_status["status"]
            assert current_state["progress"] == expected_status["progress"]
    
    @pytest.mark.asyncio
    async def test_error_handling_during_execution(self, sample_team_config, mock_key_manager,
                                                  mock_state_manager, mock_event_manager):
        """测试执行过程中的错误处理"""
        # 构建团队
        manager = HierarchicalManager(key_manager=mock_key_manager)
        team = manager.build_hierarchy(sample_team_config)
        
        # 创建执行器
        executor = HierarchicalExecutor(
            state_manager=mock_state_manager,
            event_manager=mock_event_manager
        )
        
        execution_id = "test_exec_error"
        
        # Mock执行错误
        with patch.object(executor, 'execute_hierarchical_team', new_callable=AsyncMock) as mock_execute:
            mock_execute.side_effect = Exception("模拟执行错误")
            
            # 验证错误处理
            with pytest.raises(Exception) as exc_info:
                await executor.execute_hierarchical_team(team, execution_id)
            
            assert "模拟执行错误" in str(exc_info.value)
            
            # 验证错误事件被发出
            mock_event_manager.emit_event.assert_called()
    
    @pytest.mark.asyncio
    async def test_concurrent_execution_support(self, sample_team_config, mock_key_manager,
                                              mock_state_manager, mock_event_manager):
        """测试并发执行支持"""
        # 构建团队
        manager = HierarchicalManager(key_manager=mock_key_manager)
        team = manager.build_hierarchy(sample_team_config)
        
        # 创建执行器
        executor = HierarchicalExecutor(
            state_manager=mock_state_manager,
            event_manager=mock_event_manager
        )
        
        # 创建多个执行任务
        execution_ids = ["exec_1", "exec_2", "exec_3"]
        
        async def mock_execute_team(team_obj, exec_id):
            # 模拟异步执行
            await asyncio.sleep(0.1)
            return {
                "execution_id": exec_id,
                "status": "completed",
                "team_results": {"research_team": {"status": "completed"}}
            }
        
        with patch.object(executor, 'execute_hierarchical_team', side_effect=mock_execute_team):
            # 并发执行多个任务
            tasks = [
                executor.execute_hierarchical_team(team, exec_id)
                for exec_id in execution_ids
            ]
            
            results = await asyncio.gather(*tasks)
            
            # 验证所有执行都完成
            assert len(results) == 3
            for i, result in enumerate(results):
                assert result["execution_id"] == execution_ids[i]
                assert result["status"] == "completed"


class TestRequirement3StandardizedOutput(TestEndToEndIntegration):
    """需求3验证：测试标准化输出和模板化报告"""
    
    @pytest.fixture
    def sample_execution_results(self):
        """示例执行结果"""
        return {
            "execution_id": "test_exec_789",
            "team_results": {
                "research_team": {
                    "status": "completed",
                    "duration": 300,
                    "agents": {
                        "search_agent": {
                            "agent_name": "搜索专家",
                            "status": "completed",
                            "output": "找到15篇相关研究论文，包括AI在医疗领域的应用",
                            "tools_used": ["search_tool"],
                            "metrics": {
                                "sources_found": 15,
                                "processing_time": 150
                            }
                        },
                        "analysis_agent": {
                            "agent_name": "分析专家", 
                            "status": "completed",
                            "output": "分析了技术趋势，识别出数据隐私等关键挑战",
                            "tools_used": ["data_processor"],
                            "metrics": {
                                "analysis_points": 8,
                                "processing_time": 120
                            }
                        }
                    }
                },
                "writing_team": {
                    "status": "completed",
                    "duration": 450,
                    "agents": {
                        "writer_agent": {
                            "agent_name": "写作专家",
                            "status": "completed",
                            "output": "完成了详细的技术分析报告，包含背景、案例、挑战和展望",
                            "tools_used": ["document_writer"],
                            "metrics": {
                                "words_written": 2500,
                                "processing_time": 400
                            }
                        }
                    }
                }
            },
            "execution_summary": {
                "status": "completed",
                "started_at": "2024-01-15T10:35:00Z",
                "completed_at": "2024-01-15T11:05:00Z",
                "total_duration": 1800,
                "teams_executed": 2,
                "agents_involved": 3
            },
            "metrics": {
                "total_tokens_used": 5000,
                "api_calls_made": 12,
                "success_rate": 1.0,
                "average_response_time": 2.5
            }
        }
    
    @pytest.mark.asyncio
    async def test_standardized_output_collection(self, sample_execution_results):
        """测试标准化输出收集"""
        # Mock输出格式化器
        formatter = Mock()
        
        # Mock标准化输出
        mock_standardized_output = Mock()
        mock_standardized_output.execution_id = "test_exec_789"
        mock_standardized_output.execution_summary.status = "completed"
        mock_standardized_output.execution_summary.teams_executed = 2
        mock_standardized_output.execution_summary.agents_involved = 3
        mock_standardized_output.execution_summary.total_duration = 1800
        
        mock_standardized_output.team_results = {
            "research_team": Mock(status="completed", duration=300, agents={"search_agent": Mock()}),
            "writing_team": Mock(status="completed", duration=450)
        }
        
        mock_standardized_output.metrics.total_tokens_used = 5000
        mock_standardized_output.metrics.api_calls_made = 12
        mock_standardized_output.metrics.success_rate = 1.0
        
        formatter.format_results.return_value = mock_standardized_output
        
        # 生成标准化输出
        standardized_output = formatter.format_results(sample_execution_results)
        
        # 验证标准化输出结构
        assert standardized_output.execution_id == "test_exec_789"
        
        # 验证执行摘要
        summary = standardized_output.execution_summary
        assert summary.status == "completed"
        assert summary.teams_executed == 2
        assert summary.agents_involved == 3
        assert summary.total_duration == 1800
        
        # 验证团队结果
        team_results = standardized_output.team_results
        assert "research_team" in team_results
        assert "writing_team" in team_results
        
        research_result = team_results["research_team"]
        assert research_result.status == "completed"
        assert research_result.duration == 300
        
        # 验证指标
        metrics = standardized_output.metrics
        assert metrics.total_tokens_used == 5000
        assert metrics.api_calls_made == 12
        assert metrics.success_rate == 1.0
    
    @pytest.mark.asyncio
    async def test_template_based_formatting(self, sample_execution_results):
        """测试基于模板的格式化"""
        formatter = OutputFormatter()
        
        # 定义输出模板
        template = {
            "report_title": "AI医疗应用分析报告",
            "executive_summary": "{从所有团队结果中提取执行摘要}",
            "research_findings": {
                "key_technologies": "{从研究团队结果中提取关键技术}",
                "market_trends": "{从分析结果中提取市场趋势}",
                "challenges": "{从分析结果中提取挑战}"
            },
            "recommendations": "{从写作团队结果中提取建议}",
            "appendix": {
                "data_sources": "{列出所有数据来源}",
                "methodology": "{描述研究方法}"
            }
        }
        
        # 定义提取规则
        extraction_rules = {
            "executive_summary": "总结所有团队的核心发现，不超过200字",
            "key_technologies": "从搜索结果中提取3-5个关键技术",
            "market_trends": "从分析结果中提取市场趋势，以列表形式呈现",
            "challenges": "识别并列出主要技术和商业挑战",
            "recommendations": "基于分析结果提供3-5条具体建议"
        }
        
        # 应用模板格式化
        formatted_output = formatter.apply_template(
            sample_execution_results, 
            template, 
            extraction_rules
        )
        
        # 验证模板化输出
        assert formatted_output["report_title"] == "AI医疗应用分析报告"
        assert "executive_summary" in formatted_output
        assert "research_findings" in formatted_output
        assert "recommendations" in formatted_output
        assert "appendix" in formatted_output
        
        # 验证嵌套结构
        research_findings = formatted_output["research_findings"]
        assert "key_technologies" in research_findings
        assert "market_trends" in research_findings
        assert "challenges" in research_findings
    
    @pytest.mark.asyncio
    async def test_partial_failure_output_handling(self):
        """测试部分失败情况的输出处理"""
        # 创建部分失败的执行结果
        partial_failure_results = {
            "execution_id": "test_exec_partial",
            "team_results": {
                "research_team": {
                    "status": "completed",
                    "duration": 300,
                    "agents": {
                        "search_agent": {
                            "status": "completed",
                            "output": "搜索完成"
                        }
                    }
                },
                "writing_team": {
                    "status": "failed",
                    "duration": 150,
                    "agents": {
                        "writer_agent": {
                            "status": "failed",
                            "output": None,
                            "error": "LLM API调用失败"
                        }
                    }
                }
            },
            "execution_summary": {
                "status": "partial_failure",
                "started_at": "2024-01-15T10:35:00Z",
                "completed_at": "2024-01-15T10:50:00Z",
                "total_duration": 900,
                "teams_executed": 1,
                "agents_involved": 2
            },
            "errors": [
                {
                    "error_code": "LLM_API_ERROR",
                    "message": "OpenAI API调用超时",
                    "timestamp": "2024-01-15T10:45:00Z",
                    "context": {"agent_id": "writer_agent", "team_id": "writing_team"}
                }
            ]
        }
        
        formatter = OutputFormatter()
        standardized_output = formatter.format_results(partial_failure_results)
        
        # 验证部分失败处理
        assert standardized_output.execution_summary.status == "partial_failure"
        assert len(standardized_output.errors) == 1
        assert standardized_output.errors[0].error_code == "LLM_API_ERROR"
        
        # 验证成功和失败的团队都被正确记录
        assert "research_team" in standardized_output.team_results
        assert "writing_team" in standardized_output.team_results
        assert standardized_output.team_results["research_team"].status == "completed"
        assert standardized_output.team_results["writing_team"].status == "failed"
    
    @pytest.mark.asyncio
    async def test_custom_output_formats(self, sample_execution_results):
        """测试自定义输出格式"""
        formatter = OutputFormatter()
        
        # 测试JSON格式
        json_output = formatter.to_json(sample_execution_results)
        assert isinstance(json_output, str)
        parsed_json = json.loads(json_output)
        assert parsed_json["execution_id"] == "test_exec_789"
        
        # 测试Markdown格式
        markdown_output = formatter.to_markdown(sample_execution_results)
        assert isinstance(markdown_output, str)
        assert "# 执行报告" in markdown_output
        assert "## 执行摘要" in markdown_output
        assert "## 团队结果" in markdown_output
        
        # 测试XML格式
        xml_output = formatter.to_xml(sample_execution_results)
        assert isinstance(xml_output, str)
        assert "<execution_report>" in xml_output
        assert "<execution_id>test_exec_789</execution_id>" in xml_output
    
    @pytest.mark.asyncio
    async def test_metrics_calculation_accuracy(self, sample_execution_results):
        """测试指标计算的准确性"""
        formatter = OutputFormatter()
        
        # 计算详细指标
        detailed_metrics = formatter.calculate_detailed_metrics(sample_execution_results)
        
        # 验证基础指标
        assert detailed_metrics["total_agents"] == 3
        assert detailed_metrics["successful_agents"] == 3
        assert detailed_metrics["failed_agents"] == 0
        assert detailed_metrics["success_rate"] == 1.0
        
        # 验证时间指标
        assert detailed_metrics["total_execution_time"] == 1800
        assert detailed_metrics["average_agent_time"] > 0
        
        # 验证工具使用统计
        tool_usage = detailed_metrics["tool_usage"]
        assert "search_tool" in tool_usage
        assert "data_processor" in tool_usage
        assert "document_writer" in tool_usage
        
        # 验证输出统计
        output_stats = detailed_metrics["output_statistics"]
        assert output_stats["total_output_length"] > 0
        assert output_stats["average_output_length"] > 0


class TestRequirement4AgentFlexibility(TestEndToEndIntegration):
    """需求4验证：测试智能体配置和工具集成的灵活性"""
    
    @pytest.mark.asyncio
    async def test_custom_agent_roles_and_tools(self, mock_key_manager):
        """测试自定义智能体角色和工具"""
        # 创建自定义智能体配置
        custom_config = {
            "team_name": "custom_agents_team",
            "description": "自定义智能体团队",
            "top_supervisor_config": {
                "llm_config": {"provider": "openai", "model": "gpt-4o"},
                "system_prompt": "顶级监督者",
                "user_prompt": "协调自定义智能体",
                "max_iterations": 10
            },
            "sub_teams": [{
                "id": "custom_team",
                "name": "自定义团队",
                "description": "包含各种自定义智能体",
                "supervisor_config": {
                    "llm_config": {"provider": "openai", "model": "gpt-4o"},
                    "system_prompt": "自定义团队监督者",
                    "user_prompt": "协调自定义任务",
                    "max_iterations": 8
                },
                "agent_configs": [
                    {
                        "agent_id": "financial_analyst",
                        "agent_name": "金融分析师",
                        "llm_config": {
                            "provider": "openai",
                            "model": "gpt-4o",
                            "temperature": 0.2,
                            "max_tokens": 3000
                        },
                        "system_prompt": "你是专业的金融分析师，擅长市场分析和投资建议。",
                        "user_prompt": "分析当前市场趋势并提供投资建议。",
                        "tools": ["financial_data_api", "market_analyzer", "risk_calculator"],
                        "max_iterations": 5
                    },
                    {
                        "agent_id": "legal_advisor",
                        "agent_name": "法律顾问",
                        "llm_config": {
                            "provider": "openai",
                            "model": "gpt-4o",
                            "temperature": 0.1,
                            "max_tokens": 4000
                        },
                        "system_prompt": "你是专业的法律顾问，精通商业法和合规要求。",
                        "user_prompt": "审查法律合规性并提供法律建议。",
                        "tools": ["legal_database", "compliance_checker", "contract_analyzer"],
                        "max_iterations": 3
                    },
                    {
                        "agent_id": "creative_designer",
                        "agent_name": "创意设计师",
                        "llm_config": {
                            "provider": "openai",
                            "model": "gpt-4o",
                            "temperature": 0.8,
                            "max_tokens": 2000
                        },
                        "system_prompt": "你是富有创意的设计师，擅长视觉设计和用户体验。",
                        "user_prompt": "创建创新的设计方案和用户界面。",
                        "tools": ["design_generator", "color_palette", "ui_mockup"],
                        "max_iterations": 7
                    }
                ]
            }],
            "dependencies": {},
            "global_config": {
                "max_execution_time": 3600,
                "enable_streaming": True,
                "output_format": "detailed"
            }
        }
        
        # 构建团队
        manager = HierarchicalManager(key_manager=mock_key_manager)
        team = manager.build_hierarchy(custom_config)
        
        # 验证自定义智能体配置
        custom_team = team.teams["custom_team"]
        assert len(custom_team.agents) == 3
        
        agents_by_name = {agent.config.agent_name: agent for agent in custom_team.agents}
        
        # 验证金融分析师配置
        financial_agent = agents_by_name["金融分析师"]
        assert financial_agent.config.llm_config.temperature == 0.2
        assert "financial_data_api" in financial_agent.config.tools
        assert "市场分析" in financial_agent.config.system_prompt
        
        # 验证法律顾问配置
        legal_agent = agents_by_name["法律顾问"]
        assert legal_agent.config.llm_config.temperature == 0.1
        assert "legal_database" in legal_agent.config.tools
        assert "法律顾问" in legal_agent.config.system_prompt
        
        # 验证创意设计师配置
        creative_agent = agents_by_name["创意设计师"]
        assert creative_agent.config.llm_config.temperature == 0.8
        assert "design_generator" in creative_agent.config.tools
        assert "创意" in creative_agent.config.system_prompt
    
    @pytest.mark.asyncio
    async def test_custom_supervisor_strategies(self, mock_key_manager):
        """测试自定义监督策略"""
        # 创建具有不同监督策略的配置
        supervisor_strategy_config = {
            "team_name": "strategy_test_team",
            "description": "监督策略测试团队",
            "top_supervisor_config": {
                "llm_config": {"provider": "openai", "model": "gpt-4o"},
                "system_prompt": "你是战略级监督者，需要基于业务优先级和资源可用性做出决策。",
                "user_prompt": "根据当前业务需求和团队能力，选择最优的执行策略。",
                "max_iterations": 15
            },
            "sub_teams": [
                {
                    "id": "priority_team",
                    "name": "优先级团队",
                    "description": "处理高优先级任务",
                    "supervisor_config": {
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "你是优先级导向的监督者，总是选择最紧急和重要的任务。",
                        "user_prompt": "评估任务紧急程度，优先处理高优先级任务。",
                        "max_iterations": 10
                    },
                    "agent_configs": [{
                        "agent_id": "urgent_handler",
                        "agent_name": "紧急处理专家",
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "专门处理紧急任务",
                        "user_prompt": "快速处理紧急任务",
                        "tools": ["priority_queue", "fast_processor"],
                        "max_iterations": 3
                    }]
                },
                {
                    "id": "quality_team",
                    "name": "质量团队",
                    "description": "注重质量的团队",
                    "supervisor_config": {
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "你是质量导向的监督者，确保输出质量达到最高标准。",
                        "user_prompt": "评估任务复杂度，确保质量优先。",
                        "max_iterations": 12
                    },
                    "agent_configs": [{
                        "agent_id": "quality_expert",
                        "agent_name": "质量专家",
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "专注于高质量输出",
                        "user_prompt": "确保输出质量",
                        "tools": ["quality_checker", "validator"],
                        "max_iterations": 8
                    }]
                }
            ],
            "dependencies": {},
            "global_config": {
                "max_execution_time": 3600,
                "enable_streaming": True,
                "output_format": "detailed"
            }
        }
        
        # 构建团队
        manager = HierarchicalManager(key_manager=mock_key_manager)
        team = manager.build_hierarchy(supervisor_strategy_config)
        
        # 验证不同的监督策略
        assert team.top_supervisor is not None
        assert "战略级监督者" in team.top_supervisor.config.system_prompt
        
        priority_team = team.teams["priority_team"]
        quality_team = team.teams["quality_team"]
        
        assert "优先级导向" in priority_team.supervisor.config.system_prompt
        assert "质量导向" in quality_team.supervisor.config.system_prompt
    
    @pytest.mark.asyncio
    async def test_dynamic_agent_addition_removal(self, mock_key_manager):
        """测试动态添加或移除智能体"""
        # 创建基础团队配置
        base_config = {
            "team_name": "dynamic_team",
            "description": "动态团队",
            "top_supervisor_config": {
                "llm_config": {"provider": "openai", "model": "gpt-4o"},
                "system_prompt": "顶级监督者",
                "user_prompt": "协调动态团队",
                "max_iterations": 10
            },
            "sub_teams": [{
                "id": "flexible_team",
                "name": "灵活团队",
                "description": "可动态调整的团队",
                "supervisor_config": {
                    "llm_config": {"provider": "openai", "model": "gpt-4o"},
                    "system_prompt": "灵活团队监督者",
                    "user_prompt": "管理动态团队成员",
                    "max_iterations": 8
                },
                "agent_configs": [{
                    "agent_id": "base_agent",
                    "agent_name": "基础智能体",
                    "llm_config": {"provider": "openai", "model": "gpt-4o"},
                    "system_prompt": "基础智能体",
                    "user_prompt": "执行基础任务",
                    "tools": ["basic_tool"],
                    "max_iterations": 5
                }]
            }],
            "dependencies": {},
            "global_config": {
                "max_execution_time": 3600,
                "enable_streaming": True,
                "output_format": "detailed"
            }
        }
        
        # 构建初始团队
        manager = HierarchicalManager(key_manager=mock_key_manager)
        team = manager.build_hierarchy(base_config)
        
        # 验证初始状态
        flexible_team = team.teams["flexible_team"]
        assert len(flexible_team.agents) == 1
        assert flexible_team.agents[0].config.agent_name == "基础智能体"
        
        # 模拟动态添加智能体
        new_agent_config = {
            "agent_id": "specialist_agent",
            "agent_name": "专家智能体",
            "llm_config": {"provider": "openai", "model": "gpt-4o"},
            "system_prompt": "专家智能体",
            "user_prompt": "执行专业任务",
            "tools": ["specialist_tool"],
            "max_iterations": 3
        }
        
        # 添加新智能体到配置
        base_config["sub_teams"][0]["agent_configs"].append(new_agent_config)
        
        # 重新构建团队
        updated_team = manager.build_hierarchy(base_config)
        updated_flexible_team = updated_team.teams["flexible_team"]
        
        # 验证智能体已添加
        assert len(updated_flexible_team.agents) == 2
        agent_names = [agent.config.agent_name for agent in updated_flexible_team.agents]
        assert "基础智能体" in agent_names
        assert "专家智能体" in agent_names
    
    @pytest.mark.asyncio
    async def test_plugin_based_extension_mechanism(self, mock_key_manager):
        """测试插件化扩展机制"""
        # 模拟插件工具
        class CustomPlugin:
            def __init__(self, name: str):
                self.name = name
            
            def execute(self, input_data: str) -> str:
                return f"Plugin {self.name} processed: {input_data}"
        
        # 创建使用插件的配置
        plugin_config = {
            "team_name": "plugin_team",
            "description": "插件扩展团队",
            "top_supervisor_config": {
                "llm_config": {"provider": "openai", "model": "gpt-4o"},
                "system_prompt": "顶级监督者",
                "user_prompt": "协调插件团队",
                "max_iterations": 10
            },
            "sub_teams": [{
                "id": "plugin_team",
                "name": "插件团队",
                "description": "使用插件扩展的团队",
                "supervisor_config": {
                    "llm_config": {"provider": "openai", "model": "gpt-4o"},
                    "system_prompt": "插件团队监督者",
                    "user_prompt": "管理插件智能体",
                    "max_iterations": 8
                },
                "agent_configs": [{
                    "agent_id": "plugin_agent",
                    "agent_name": "插件智能体",
                    "llm_config": {"provider": "openai", "model": "gpt-4o"},
                    "system_prompt": "使用插件扩展功能的智能体",
                    "user_prompt": "使用插件处理任务",
                    "tools": ["custom_plugin_1", "custom_plugin_2", "standard_tool"],
                    "max_iterations": 5
                }]
            }],
            "dependencies": {},
            "global_config": {
                "max_execution_time": 3600,
                "enable_streaming": True,
                "output_format": "detailed"
            }
        }
        
        # 构建团队
        manager = HierarchicalManager(key_manager=mock_key_manager)
        team = manager.build_hierarchy(plugin_config)
        
        # 验证插件工具配置
        plugin_team = team.teams["plugin_team"]
        plugin_agent = plugin_team.agents[0]
        
        assert "custom_plugin_1" in plugin_agent.config.tools
        assert "custom_plugin_2" in plugin_agent.config.tools
        assert "standard_tool" in plugin_agent.config.tools
        
        # 验证插件可以被正确加载和使用
        assert plugin_agent.config.agent_name == "插件智能体"
        assert "插件扩展功能" in plugin_agent.config.system_prompt
    
    @pytest.mark.asyncio
    async def test_multi_llm_provider_flexibility(self, mock_key_manager):
        """测试多LLM提供商的灵活性"""
        # 创建使用不同LLM提供商的配置
        multi_llm_config = {
            "team_name": "multi_llm_team",
            "description": "多LLM提供商团队",
            "top_supervisor_config": {
                "llm_config": {
                    "provider": "openai",
                    "model": "gpt-4o",
                    "temperature": 0.3
                },
                "system_prompt": "顶级监督者",
                "user_prompt": "协调多LLM团队",
                "max_iterations": 10
            },
            "sub_teams": [{
                "id": "multi_llm_team",
                "name": "多LLM团队",
                "description": "使用不同LLM提供商的团队",
                "supervisor_config": {
                    "llm_config": {
                        "provider": "openrouter",
                        "model": "anthropic/claude-3-sonnet",
                        "base_url": "https://openrouter.ai/api/v1",
                        "temperature": 0.5
                    },
                    "system_prompt": "多LLM团队监督者",
                    "user_prompt": "管理不同LLM智能体",
                    "max_iterations": 8
                },
                "agent_configs": [
                    {
                        "agent_id": "openai_agent",
                        "agent_name": "OpenAI智能体",
                        "llm_config": {
                            "provider": "openai",
                            "model": "gpt-4o",
                            "temperature": 0.7,
                            "max_tokens": 2000
                        },
                        "system_prompt": "使用OpenAI的智能体",
                        "user_prompt": "执行OpenAI任务",
                        "tools": ["openai_tool"],
                        "max_iterations": 5
                    },
                    {
                        "agent_id": "claude_agent",
                        "agent_name": "Claude智能体",
                        "llm_config": {
                            "provider": "openrouter",
                            "model": "anthropic/claude-3-sonnet",
                            "base_url": "https://openrouter.ai/api/v1",
                            "temperature": 0.3,
                            "max_tokens": 3000
                        },
                        "system_prompt": "使用Claude的智能体",
                        "user_prompt": "执行Claude任务",
                        "tools": ["claude_tool"],
                        "max_iterations": 3
                    },
                    {
                        "agent_id": "bedrock_agent",
                        "agent_name": "Bedrock智能体",
                        "llm_config": {
                            "provider": "aws_bedrock",
                            "model": "anthropic.claude-3-sonnet-20240229-v1:0",
                            "region": "us-east-1",
                            "temperature": 0.4
                        },
                        "system_prompt": "使用AWS Bedrock的智能体",
                        "user_prompt": "执行Bedrock任务",
                        "tools": ["bedrock_tool"],
                        "max_iterations": 4
                    }
                ]
            }],
            "dependencies": {},
            "global_config": {
                "max_execution_time": 3600,
                "enable_streaming": True,
                "output_format": "detailed"
            }
        }
        
        # 构建团队
        manager = HierarchicalManager(key_manager=mock_key_manager)
        team = manager.build_hierarchy(multi_llm_config)
        
        # 验证不同LLM提供商配置
        multi_llm_team = team.teams["multi_llm_team"]
        assert len(multi_llm_team.agents) == 3
        
        agents_by_name = {agent.config.agent_name: agent for agent in multi_llm_team.agents}
        
        # 验证OpenAI智能体
        openai_agent = agents_by_name["OpenAI智能体"]
        assert openai_agent.config.llm_config.provider == "openai"
        assert openai_agent.config.llm_config.model == "gpt-4o"
        
        # 验证Claude智能体
        claude_agent = agents_by_name["Claude智能体"]
        assert claude_agent.config.llm_config.provider == "openrouter"
        assert "claude" in claude_agent.config.llm_config.model
        
        # 验证Bedrock智能体
        bedrock_agent = agents_by_name["Bedrock智能体"]
        assert bedrock_agent.config.llm_config.provider == "aws_bedrock"
        assert "bedrock" in bedrock_agent.config.llm_config.model or "claude" in bedrock_agent.config.llm_config.model
        
        # 验证监督者使用OpenRouter
        supervisor = multi_llm_team.supervisor
        assert supervisor.config.llm_config.provider == "openrouter"
        
        # 验证顶级监督者使用OpenAI
        top_supervisor = team.top_supervisor
        assert top_supervisor.config.llm_config.provider == "openai"


class TestRequirement5ErrorHandlingAndStability(TestEndToEndIntegration):
    """需求5验证：测试错误处理和系统稳定性"""
    
    @pytest.mark.asyncio
    async def test_agent_execution_failure_handling(self, sample_team_config, mock_key_manager,
                                                   mock_state_manager, mock_event_manager):
        """测试智能体执行失败的处理"""
        # 构建团队
        manager = HierarchicalManager(key_manager=mock_key_manager)
        team = manager.build_hierarchy(sample_team_config)
        
        # 创建执行器
        executor = HierarchicalExecutor(
            state_manager=mock_state_manager,
            event_manager=mock_event_manager
        )
        
        execution_id = "test_exec_error"
        
        # Mock智能体执行失败
        with patch.object(executor, '_execute_agent', new_callable=AsyncMock) as mock_execute_agent:
            mock_execute_agent.side_effect = Exception("智能体执行失败")
            
            # 执行应该处理错误并继续
            with patch.object(executor, '_handle_agent_error', new_callable=AsyncMock) as mock_handle_error:
                mock_handle_error.return_value = {
                    "status": "failed",
                    "error": "智能体执行失败",
                    "recovery_attempted": True
                }
                
                try:
                    result = await executor.execute_hierarchical_team(team, execution_id)
                    # 验证错误被正确处理
                    mock_handle_error.assert_called()
                except Exception:
                    # 如果异常传播，验证错误处理被调用
                    mock_handle_error.assert_called()
    
    @pytest.mark.asyncio
    async def test_network_error_retry_mechanism(self, mock_key_manager):
        """测试网络错误的重试机制"""
        from hierarchical_agents.error_handler import ErrorHandler
        
        # 创建错误处理器
        error_handler = ErrorHandler()
        
        # 模拟网络错误
        network_errors = [
            Exception("Connection timeout"),
            Exception("Network unreachable"),
            Exception("DNS resolution failed")
        ]
        
        for error in network_errors:
            # 测试错误分类
            error_type = error_handler.classify_error(error)
            assert error_type in ["network_error", "recoverable_error"]
            
            # 测试重试决策
            should_retry = error_handler.should_retry(error, attempt=1)
            assert should_retry is True
            
            # 测试重试次数限制
            should_retry_max = error_handler.should_retry(error, attempt=5)
            assert should_retry_max is False
    
    @pytest.mark.asyncio
    async def test_supervisor_failure_reassignment(self, sample_team_config, mock_key_manager,
                                                  mock_event_manager):
        """测试监督者失效时的重分配"""
        # 构建团队
        manager = HierarchicalManager(key_manager=mock_key_manager)
        team = manager.build_hierarchy(sample_team_config)
        
        # 创建执行器
        executor = HierarchicalExecutor(
            state_manager=mock_state_manager,
            event_manager=mock_event_manager
        )
        
        execution_id = "test_supervisor_failure"
        
        # Mock监督者失效
        original_supervisor = team.teams["research_team"].supervisor
        
        with patch.object(executor, '_execute_supervisor', new_callable=AsyncMock) as mock_execute_supervisor:
            mock_execute_supervisor.side_effect = Exception("监督者失效")
            
            # Mock监督者重分配
            with patch.object(executor, '_reassign_supervisor', new_callable=AsyncMock) as mock_reassign:
                backup_supervisor = Mock()
                backup_supervisor.config = original_supervisor.config
                mock_reassign.return_value = backup_supervisor
                
                # 执行应该处理监督者失效
                try:
                    await executor.execute_hierarchical_team(team, execution_id)
                    # 验证重分配被调用
                    mock_reassign.assert_called()
                except Exception:
                    # 即使失败，也应该尝试重分配
                    mock_reassign.assert_called()
    
    @pytest.mark.asyncio
    async def test_graceful_system_shutdown(self, sample_team_config, mock_key_manager,
                                          mock_state_manager, mock_event_manager):
        """测试系统优雅停止"""
        # 构建团队
        manager = HierarchicalManager(key_manager=mock_key_manager)
        team = manager.build_hierarchy(sample_team_config)
        
        # 创建执行引擎
        execution_engine = ExecutionEngine(
            state_manager=mock_state_manager,
            event_manager=mock_event_manager
        )
        
        execution_id = "test_graceful_shutdown"
        
        # 模拟长时间运行的执行
        async def long_running_execution():
            await asyncio.sleep(2)  # 模拟长时间执行
            return {"status": "completed"}
        
        with patch.object(execution_engine, 'execute', side_effect=long_running_execution):
            # 启动执行
            execution_task = asyncio.create_task(
                execution_engine.execute(team, execution_id)
            )
            
            # 等待一小段时间后取消
            await asyncio.sleep(0.1)
            execution_task.cancel()
            
            # 验证优雅停止
            try:
                await execution_task
            except asyncio.CancelledError:
                # 验证状态被正确更新
                mock_state_manager.update_execution_state.assert_called()
                # 验证停止事件被发出
                mock_event_manager.emit_event.assert_called()
    
    @pytest.mark.asyncio
    async def test_partial_results_return(self, mock_key_manager, mock_state_manager, mock_event_manager):
        """测试部分结果返回"""
        # 创建复杂团队配置
        complex_config = {
            "team_name": "complex_team",
            "description": "复杂团队",
            "top_supervisor_config": {
                "llm_config": {"provider": "openai", "model": "gpt-4o"},
                "system_prompt": "顶级监督者",
                "user_prompt": "协调复杂团队",
                "max_iterations": 10
            },
            "sub_teams": [
                {
                    "id": "team_1",
                    "name": "团队1",
                    "description": "第一个团队",
                    "supervisor_config": {
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "团队1监督者",
                        "user_prompt": "执行任务1",
                        "max_iterations": 5
                    },
                    "agent_configs": [{
                        "agent_id": "agent_1",
                        "agent_name": "智能体1",
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "智能体1",
                        "user_prompt": "执行子任务1",
                        "tools": [],
                        "max_iterations": 3
                    }]
                },
                {
                    "id": "team_2",
                    "name": "团队2",
                    "description": "第二个团队",
                    "supervisor_config": {
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "团队2监督者",
                        "user_prompt": "执行任务2",
                        "max_iterations": 5
                    },
                    "agent_configs": [{
                        "agent_id": "agent_2",
                        "agent_name": "智能体2",
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "智能体2",
                        "user_prompt": "执行子任务2",
                        "tools": [],
                        "max_iterations": 3
                    }]
                },
                {
                    "id": "team_3",
                    "name": "团队3",
                    "description": "第三个团队",
                    "supervisor_config": {
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "团队3监督者",
                        "user_prompt": "执行任务3",
                        "max_iterations": 5
                    },
                    "agent_configs": [{
                        "agent_id": "agent_3",
                        "agent_name": "智能体3",
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "智能体3",
                        "user_prompt": "执行子任务3",
                        "tools": [],
                        "max_iterations": 3
                    }]
                }
            ],
            "dependencies": {"team_2": ["team_1"], "team_3": ["team_2"]},
            "global_config": {
                "max_execution_time": 3600,
                "enable_streaming": True,
                "output_format": "detailed"
            }
        }
        
        # 构建团队
        manager = HierarchicalManager(key_manager=mock_key_manager)
        team = manager.build_hierarchy(complex_config)
        
        # 创建执行器
        executor = HierarchicalExecutor(
            state_manager=mock_state_manager,
            event_manager=mock_event_manager
        )
        
        execution_id = "test_partial_results"
        
        # Mock部分成功的执行
        def mock_team_execution(team_id):
            if team_id == "team_1":
                return {"status": "completed", "output": "团队1完成"}
            elif team_id == "team_2":
                return {"status": "completed", "output": "团队2完成"}
            elif team_id == "team_3":
                raise Exception("团队3执行失败")
        
        with patch.object(executor, '_execute_team', side_effect=mock_team_execution):
            try:
                result = await executor.execute_hierarchical_team(team, execution_id)
                
                # 验证部分结果被返回
                assert "team_1" in result.get("team_results", {})
                assert "team_2" in result.get("team_results", {})
                assert result["team_results"]["team_1"]["status"] == "completed"
                assert result["team_results"]["team_2"]["status"] == "completed"
                
            except Exception:
                # 即使有异常，也应该能获取部分结果
                partial_results = await executor.get_partial_results(execution_id)
                assert partial_results is not None
                assert len(partial_results) >= 2  # 至少有两个团队的结果
    
    @pytest.mark.asyncio
    async def test_detailed_error_logging(self, sample_team_config, mock_key_manager):
        """测试详细错误日志记录"""
        from hierarchical_agents.logging_monitor import LoggingMonitor
        
        # 创建日志监控器
        logger = LoggingMonitor()
        
        # 模拟各种错误场景
        error_scenarios = [
            {
                "error": Exception("LLM API调用失败"),
                "context": {"agent_id": "test_agent", "team_id": "test_team"},
                "expected_level": "ERROR"
            },
            {
                "error": TimeoutError("执行超时"),
                "context": {"execution_id": "test_exec", "timeout": 30},
                "expected_level": "WARNING"
            },
            {
                "error": ValueError("配置参数无效"),
                "context": {"config_field": "llm_config", "value": "invalid"},
                "expected_level": "ERROR"
            }
        ]
        
        for scenario in error_scenarios:
            # 记录错误
            logger.log_error(
                scenario["error"],
                context=scenario["context"],
                level=scenario["expected_level"]
            )
            
            # 验证错误被正确记录
            assert logger.has_error_logged(str(scenario["error"]))
            
            # 验证上下文信息被包含
            log_entry = logger.get_last_log_entry()
            assert log_entry["level"] == scenario["expected_level"]
            assert "context" in log_entry
    
    @pytest.mark.asyncio
    async def test_system_recovery_mechanisms(self, sample_team_config, mock_key_manager,
                                            mock_state_manager, mock_event_manager):
        """测试系统恢复机制"""
        from hierarchical_agents.error_handler import ErrorHandler
        
        # 创建错误处理器
        error_handler = ErrorHandler()
        
        # 测试不同的恢复策略
        recovery_scenarios = [
            {
                "error_type": "agent_failure",
                "recovery_strategy": "restart_agent",
                "expected_action": "重启智能体"
            },
            {
                "error_type": "network_timeout",
                "recovery_strategy": "retry_with_backoff",
                "expected_action": "指数退避重试"
            },
            {
                "error_type": "supervisor_failure",
                "recovery_strategy": "reassign_supervisor",
                "expected_action": "重分配监督者"
            },
            {
                "error_type": "resource_exhaustion",
                "recovery_strategy": "graceful_degradation",
                "expected_action": "优雅降级"
            }
        ]
        
        for scenario in recovery_scenarios:
            # 获取恢复策略
            recovery_action = error_handler.get_recovery_strategy(scenario["error_type"])
            assert recovery_action == scenario["recovery_strategy"]
            
            # 执行恢复
            recovery_result = await error_handler.execute_recovery(
                scenario["error_type"],
                context={"execution_id": "test_recovery"}
            )
            
            # 验证恢复结果
            assert recovery_result["action"] == scenario["expected_action"]
            assert recovery_result["success"] is True
    
    @pytest.mark.asyncio
    async def test_concurrent_error_handling(self, mock_key_manager, mock_event_manager):
        """测试并发错误处理"""
        from hierarchical_agents.error_handler import ErrorHandler
        
        # 创建错误处理器
        error_handler = ErrorHandler()
        
        # 模拟并发错误
        concurrent_errors = [
            Exception("错误1"),
            Exception("错误2"),
            Exception("错误3"),
            Exception("错误4"),
            Exception("错误5")
        ]
        
        # 并发处理错误
        async def handle_error(error, index):
            await asyncio.sleep(0.1)  # 模拟处理时间
            return await error_handler.handle_error(
                error,
                context={"error_index": index}
            )
        
        # 启动并发错误处理
        tasks = [
            handle_error(error, i)
            for i, error in enumerate(concurrent_errors)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # 验证所有错误都被处理
        assert len(results) == 5
        for i, result in enumerate(results):
            if not isinstance(result, Exception):
                assert result["handled"] is True
                assert result["context"]["error_index"] == i


class TestPerformanceUnderLoad(TestEndToEndIntegration):
    """性能测试：验证系统在负载下的表现"""
    
    @pytest.mark.asyncio
    async def test_concurrent_team_execution_performance(self, mock_key_manager, 
                                                       mock_state_manager, mock_event_manager):
        """测试并发团队执行性能"""
        import time
        
        # 创建多个团队配置
        team_configs = []
        for i in range(10):  # 创建10个团队
            config = {
                "team_name": f"performance_team_{i}",
                "description": f"性能测试团队{i}",
                "top_supervisor_config": {
                    "llm_config": {"provider": "openai", "model": "gpt-4o"},
                    "system_prompt": f"团队{i}监督者",
                    "user_prompt": f"协调团队{i}",
                    "max_iterations": 5
                },
                "sub_teams": [{
                    "id": f"sub_team_{i}",
                    "name": f"子团队{i}",
                    "description": f"子团队{i}描述",
                    "supervisor_config": {
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": f"子团队{i}监督者",
                        "user_prompt": f"执行任务{i}",
                        "max_iterations": 3
                    },
                    "agent_configs": [{
                        "agent_id": f"agent_{i}",
                        "agent_name": f"智能体{i}",
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": f"智能体{i}",
                        "user_prompt": f"执行子任务{i}",
                        "tools": [],
                        "max_iterations": 2
                    }]
                }],
                "dependencies": {},
                "global_config": {
                    "max_execution_time": 300,
                    "enable_streaming": True,
                    "output_format": "summary"
                }
            }
            team_configs.append(config)
        
        # 构建所有团队
        manager = HierarchicalManager(key_manager=mock_key_manager)
        teams = [manager.build_hierarchy(config) for config in team_configs]
        
        # 创建执行器
        executor = HierarchicalExecutor(
            state_manager=mock_state_manager,
            event_manager=mock_event_manager
        )
        
        # Mock快速执行
        async def mock_fast_execution(team, execution_id):
            await asyncio.sleep(0.01)  # 模拟快速执行
            return {
                "execution_id": execution_id,
                "status": "completed",
                "team_results": {
                    list(team.teams.keys())[0]: {
                        "status": "completed",
                        "duration": 10
                    }
                }
            }
        
        with patch.object(executor, 'execute_hierarchical_team', side_effect=mock_fast_execution):
            # 测量并发执行时间
            start_time = time.time()
            
            # 并发执行所有团队
            tasks = [
                executor.execute_hierarchical_team(team, f"exec_{i}")
                for i, team in enumerate(teams)
            ]
            
            results = await asyncio.gather(*tasks)
            
            end_time = time.time()
            execution_time = end_time - start_time
            
            # 验证性能指标
            assert len(results) == 10
            assert execution_time < 1.0  # 并发执行应该在1秒内完成
            assert all(result["status"] == "completed" for result in results)
            
            # 计算吞吐量
            throughput = len(results) / execution_time
            assert throughput > 10  # 每秒至少处理10个团队
    
    @pytest.mark.asyncio
    async def test_memory_usage_under_load(self, mock_key_manager):
        """测试负载下的内存使用"""
        import psutil
        import gc
        
        # 获取初始内存使用
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # 创建大量团队
        teams = []
        manager = HierarchicalManager(key_manager=mock_key_manager)
        
        for i in range(50):  # 创建50个团队
            config = {
                "team_name": f"memory_test_team_{i}",
                "description": f"内存测试团队{i}",
                "top_supervisor_config": {
                    "llm_config": {"provider": "openai", "model": "gpt-4o"},
                    "system_prompt": "监督者",
                    "user_prompt": "协调执行",
                    "max_iterations": 5
                },
                "sub_teams": [{
                    "id": f"team_{i}",
                    "name": f"团队{i}",
                    "description": "测试团队",
                    "supervisor_config": {
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "团队监督者",
                        "user_prompt": "执行任务",
                        "max_iterations": 3
                    },
                    "agent_configs": [
                        {
                            "agent_id": f"agent_{i}_{j}",
                            "agent_name": f"智能体{i}-{j}",
                            "llm_config": {"provider": "openai", "model": "gpt-4o"},
                            "system_prompt": "智能体",
                            "user_prompt": "执行子任务",
                            "tools": [],
                            "max_iterations": 2
                        }
                        for j in range(5)  # 每个团队5个智能体
                    ]
                }],
                "dependencies": {},
                "global_config": {
                    "max_execution_time": 300,
                    "enable_streaming": False,
                    "output_format": "minimal"
                }
            }
            
            team = manager.build_hierarchy(config)
            teams.append(team)
        
        # 获取峰值内存使用
        peak_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = peak_memory - initial_memory
        
        # 验证内存使用合理
        assert memory_increase < 500  # 内存增长不应超过500MB
        assert len(teams) == 50
        
        # 清理并验证内存回收
        teams.clear()
        gc.collect()
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_recovered = peak_memory - final_memory
        
        # 验证内存回收
        assert memory_recovered > memory_increase * 0.5  # 至少回收50%的内存
    
    @pytest.mark.asyncio
    async def test_response_time_under_load(self, mock_key_manager, mock_event_manager):
        """测试负载下的响应时间"""
        import time
        import statistics
        
        # 创建执行器
        executor = HierarchicalExecutor(
            state_manager=mock_state_manager,
            event_manager=mock_event_manager
        )
        
        # 创建测试团队
        test_config = {
            "team_name": "response_time_team",
            "description": "响应时间测试团队",
            "top_supervisor_config": {
                "llm_config": {"provider": "openai", "model": "gpt-4o"},
                "system_prompt": "监督者",
                "user_prompt": "协调执行",
                "max_iterations": 5
            },
            "sub_teams": [{
                "id": "response_team",
                "name": "响应团队",
                "description": "测试响应时间",
                "supervisor_config": {
                    "llm_config": {"provider": "openai", "model": "gpt-4o"},
                    "system_prompt": "团队监督者",
                    "user_prompt": "执行任务",
                    "max_iterations": 3
                },
                "agent_configs": [{
                    "agent_id": "response_agent",
                    "agent_name": "响应智能体",
                    "llm_config": {"provider": "openai", "model": "gpt-4o"},
                    "system_prompt": "智能体",
                    "user_prompt": "执行任务",
                    "tools": [],
                    "max_iterations": 2
                }]
            }],
            "dependencies": {},
            "global_config": {
                "max_execution_time": 60,
                "enable_streaming": True,
                "output_format": "summary"
            }
        }
        
        manager = HierarchicalManager(key_manager=mock_key_manager)
        team = manager.build_hierarchy(test_config)
        
        # Mock执行时间变化
        response_times = []
        
        async def mock_variable_execution(team_obj, execution_id):
            # 模拟不同的响应时间
            delay = 0.01 + (len(response_times) * 0.001)  # 逐渐增加延迟
            await asyncio.sleep(delay)
            return {"execution_id": execution_id, "status": "completed"}
        
        with patch.object(executor, 'execute_hierarchical_team', side_effect=mock_variable_execution):
            # 执行多次测试
            for i in range(20):
                start_time = time.time()
                await executor.execute_hierarchical_team(team, f"response_test_{i}")
                end_time = time.time()
                response_times.append(end_time - start_time)
        
        # 分析响应时间
        avg_response_time = statistics.mean(response_times)
        max_response_time = max(response_times)
        min_response_time = min(response_times)
        std_deviation = statistics.stdev(response_times)
        
        # 验证响应时间指标
        assert avg_response_time < 0.1  # 平均响应时间小于100ms
        assert max_response_time < 0.2  # 最大响应时间小于200ms
        assert std_deviation < 0.05  # 标准差小于50ms，确保稳定性
    
    @pytest.mark.asyncio
    async def test_throughput_scalability(self, mock_key_manager, mock_event_manager):
        """测试吞吐量可扩展性"""
        import time
        
        # 创建执行器
        executor = HierarchicalExecutor(
            state_manager=mock_state_manager,
            event_manager=mock_event_manager
        )
        
        # 测试不同负载级别的吞吐量
        load_levels = [1, 5, 10, 20, 50]
        throughput_results = {}
        
        for load_level in load_levels:
            # 创建指定数量的团队
            teams = []
            manager = HierarchicalManager(key_manager=mock_key_manager)
            
            for i in range(load_level):
                config = {
                    "team_name": f"throughput_team_{i}",
                    "description": f"吞吐量测试团队{i}",
                    "top_supervisor_config": {
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "监督者",
                        "user_prompt": "协调执行",
                        "max_iterations": 3
                    },
                    "sub_teams": [{
                        "id": f"throughput_sub_{i}",
                        "name": f"子团队{i}",
                        "description": "吞吐量测试",
                        "supervisor_config": {
                            "llm_config": {"provider": "openai", "model": "gpt-4o"},
                            "system_prompt": "团队监督者",
                            "user_prompt": "执行任务",
                            "max_iterations": 2
                        },
                        "agent_configs": [{
                            "agent_id": f"throughput_agent_{i}",
                            "agent_name": f"吞吐量智能体{i}",
                            "llm_config": {"provider": "openai", "model": "gpt-4o"},
                            "system_prompt": "智能体",
                            "user_prompt": "执行任务",
                            "tools": [],
                            "max_iterations": 1
                        }]
                    }],
                    "dependencies": {},
                    "global_config": {
                        "max_execution_time": 30,
                        "enable_streaming": False,
                        "output_format": "minimal"
                    }
                }
                teams.append(manager.build_hierarchy(config))
            
            # Mock快速执行
            async def mock_throughput_execution(team, execution_id):
                await asyncio.sleep(0.005)  # 5ms执行时间
                return {"execution_id": execution_id, "status": "completed"}
            
            with patch.object(executor, 'execute_hierarchical_team', side_effect=mock_throughput_execution):
                # 测量吞吐量
                start_time = time.time()
                
                tasks = [
                    executor.execute_hierarchical_team(team, f"throughput_{load_level}_{i}")
                    for i, team in enumerate(teams)
                ]
                
                results = await asyncio.gather(*tasks)
                
                end_time = time.time()
                execution_time = end_time - start_time
                
                # 计算吞吐量
                throughput = len(results) / execution_time
                throughput_results[load_level] = throughput
                
                # 验证所有执行成功
                assert all(result["status"] == "completed" for result in results)
        
        # 验证可扩展性
        assert throughput_results[1] > 0
        assert throughput_results[5] > throughput_results[1] * 0.8  # 5倍负载时吞吐量至少是80%
        assert throughput_results[10] > throughput_results[5] * 0.7  # 保持合理的扩展性
        
        # 验证在高负载下仍能维持性能
        assert throughput_results[50] > 50  # 50个并发团队时仍能维持每秒50+的吞吐量
    
    @pytest.mark.asyncio
    async def test_resource_cleanup_under_load(self, mock_key_manager, mock_event_manager):
        """测试负载下的资源清理"""
        import gc
        import weakref
        
        # 创建执行器
        executor = HierarchicalExecutor(
            state_manager=mock_state_manager,
            event_manager=mock_event_manager
        )
        
        # 跟踪对象引用
        team_refs = []
        
        # 创建并执行多个团队
        for i in range(20):
            config = {
                "team_name": f"cleanup_team_{i}",
                "description": f"清理测试团队{i}",
                "top_supervisor_config": {
                    "llm_config": {"provider": "openai", "model": "gpt-4o"},
                    "system_prompt": "监督者",
                    "user_prompt": "协调执行",
                    "max_iterations": 3
                },
                "sub_teams": [{
                    "id": f"cleanup_sub_{i}",
                    "name": f"清理子团队{i}",
                    "description": "清理测试",
                    "supervisor_config": {
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "团队监督者",
                        "user_prompt": "执行任务",
                        "max_iterations": 2
                    },
                    "agent_configs": [{
                        "agent_id": f"cleanup_agent_{i}",
                        "agent_name": f"清理智能体{i}",
                        "llm_config": {"provider": "openai", "model": "gpt-4o"},
                        "system_prompt": "智能体",
                        "user_prompt": "执行任务",
                        "tools": [],
                        "max_iterations": 1
                    }]
                }],
                "dependencies": {},
                "global_config": {
                    "max_execution_time": 30,
                    "enable_streaming": False,
                    "output_format": "minimal"
                }
            }
            
            manager = HierarchicalManager(key_manager=mock_key_manager)
            team = manager.build_hierarchy(config)
            
            # 创建弱引用跟踪对象
            team_refs.append(weakref.ref(team))
            
            # Mock执行
            async def mock_cleanup_execution(team_obj, execution_id):
                await asyncio.sleep(0.001)
                return {"execution_id": execution_id, "status": "completed"}
            
            with patch.object(executor, 'execute_hierarchical_team', side_effect=mock_cleanup_execution):
                await executor.execute_hierarchical_team(team, f"cleanup_exec_{i}")
            
            # 删除团队引用
            del team
        
        # 强制垃圾回收
        gc.collect()
        
        # 验证对象被正确清理
        alive_objects = sum(1 for ref in team_refs if ref() is not None)
        cleanup_rate = (len(team_refs) - alive_objects) / len(team_refs)
        
        # 至少90%的对象应该被清理
        assert cleanup_rate >= 0.9
        assert alive_objects <= 2  # 最多只有2个对象未被清理


class TestLLMProviderCompatibility(TestEndToEndIntegration):
    """兼容性测试：测试不同LLM提供商的兼容性"""
    
    @pytest.mark.asyncio
    async def test_openai_provider_integration(self, mock_state_manager, mock_event_manager):
        """测试OpenAI提供商集成"""
        from hierarchical_agents.env_key_manager import EnvironmentKeyManager
        
        # 创建OpenAI密钥管理器
        key_manager = EnvironmentKeyManager()
        
        # Mock OpenAI环境变量
        with patch.dict(os.environ, {'OPENAI_API_KEY': 'sk-test-openai-key'}):
            # 验证密钥读取
            api_key = key_manager.get_key("openai")
            assert api_key == "sk-test-openai-key"
            
            # 验证密钥格式
            assert key_manager.validate_key_format("openai", api_key)
            
            # Mock LLM客户端创建
            with patch.object(key_manager, 'get_llm_client') as mock_get_client:
                mock_llm = Mock()
                mock_llm.invoke = Mock(return_value=Mock(content="OpenAI响应"))
                mock_get_client.return_value = mock_llm
                
                # 创建OpenAI配置
                openai_config = {
                    "team_name": "openai_team",
                    "description": "OpenAI测试团队",
                    "top_supervisor_config": {
                        "llm_config": {
                            "provider": "openai",
                            "model": "gpt-4o",
                            "temperature": 0.7,
                            "max_tokens": 2000
                        },
                        "system_prompt": "OpenAI监督者",
                        "user_prompt": "使用OpenAI协调团队",
                        "max_iterations": 10
                    },
                    "sub_teams": [{
                        "id": "openai_sub_team",
                        "name": "OpenAI子团队",
                        "description": "使用OpenAI的子团队",
                        "supervisor_config": {
                            "llm_config": {
                                "provider": "openai",
                                "model": "gpt-4o",
                                "temperature": 0.5
                            },
                            "system_prompt": "OpenAI子团队监督者",
                            "user_prompt": "使用OpenAI执行任务",
                            "max_iterations": 5
                        },
                        "agent_configs": [{
                            "agent_id": "openai_agent",
                            "agent_name": "OpenAI智能体",
                            "llm_config": {
                                "provider": "openai",
                                "model": "gpt-4o",
                                "temperature": 0.3,
                                "max_tokens": 1500
                            },
                            "system_prompt": "OpenAI智能体",
                            "user_prompt": "使用OpenAI执行子任务",
                            "tools": [],
                            "max_iterations": 3
                        }]
                    }],
                    "dependencies": {},
                    "global_config": {
                        "max_execution_time": 3600,
                        "enable_streaming": True,
                        "output_format": "detailed"
                    }
                }
                
                # 构建团队
                manager = HierarchicalManager(key_manager=key_manager)
                team = manager.build_hierarchy(openai_config)
                
                # 验证OpenAI配置
                assert team.top_supervisor.config.llm_config.provider == "openai"
                assert team.top_supervisor.config.llm_config.model == "gpt-4o"
                
                sub_team = team.teams["openai_sub_team"]
                assert sub_team.supervisor.config.llm_config.provider == "openai"
                assert sub_team.agents[0].config.llm_config.provider == "openai"
                
                # 验证LLM客户端调用
                mock_get_client.assert_called()
    
    @pytest.mark.asyncio
    async def test_openrouter_provider_integration(self, mock_state_manager, mock_event_manager):
        """测试OpenRouter提供商集成"""
        from hierarchical_agents.env_key_manager import EnvironmentKeyManager
        
        # 创建OpenRouter密钥管理器
        key_manager = EnvironmentKeyManager()
        
        # Mock OpenRouter环境变量
        with patch.dict(os.environ, {'OPENROUTER_API_KEY': 'sk-or-test-openrouter-key'}):
            # 验证密钥读取
            api_key = key_manager.get_key("openrouter")
            assert api_key == "sk-or-test-openrouter-key"
            
            # 验证密钥格式
            assert key_manager.validate_key_format("openrouter", api_key)
            
            # Mock LLM客户端创建
            with patch.object(key_manager, 'get_llm_client') as mock_get_client:
                mock_llm = Mock()
                mock_llm.invoke = Mock(return_value=Mock(content="OpenRouter响应"))
                mock_get_client.return_value = mock_llm
                
                # 创建OpenRouter配置
                openrouter_config = {
                    "team_name": "openrouter_team",
                    "description": "OpenRouter测试团队",
                    "top_supervisor_config": {
                        "llm_config": {
                            "provider": "openrouter",
                            "model": "anthropic/claude-3-sonnet",
                            "base_url": "https://openrouter.ai/api/v1",
                            "temperature": 0.5,
                            "max_tokens": 3000
                        },
                        "system_prompt": "OpenRouter监督者",
                        "user_prompt": "使用OpenRouter协调团队",
                        "max_iterations": 10
                    },
                    "sub_teams": [{
                        "id": "openrouter_sub_team",
                        "name": "OpenRouter子团队",
                        "description": "使用OpenRouter的子团队",
                        "supervisor_config": {
                            "llm_config": {
                                "provider": "openrouter",
                                "model": "anthropic/claude-3-haiku",
                                "base_url": "https://openrouter.ai/api/v1",
                                "temperature": 0.3
                            },
                            "system_prompt": "OpenRouter子团队监督者",
                            "user_prompt": "使用OpenRouter执行任务",
                            "max_iterations": 5
                        },
                        "agent_configs": [{
                            "agent_id": "openrouter_agent",
                            "agent_name": "OpenRouter智能体",
                            "llm_config": {
                                "provider": "openrouter",
                                "model": "meta-llama/llama-3.1-8b-instruct",
                                "base_url": "https://openrouter.ai/api/v1",
                                "temperature": 0.7,
                                "max_tokens": 2000
                            },
                            "system_prompt": "OpenRouter智能体",
                            "user_prompt": "使用OpenRouter执行子任务",
                            "tools": [],
                            "max_iterations": 3
                        }]
                    }],
                    "dependencies": {},
                    "global_config": {
                        "max_execution_time": 3600,
                        "enable_streaming": True,
                        "output_format": "detailed"
                    }
                }
                
                # 构建团队
                manager = HierarchicalManager(key_manager=key_manager)
                team = manager.build_hierarchy(openrouter_config)
                
                # 验证OpenRouter配置
                assert team.top_supervisor.config.llm_config.provider == "openrouter"
                assert "claude" in team.top_supervisor.config.llm_config.model
                assert team.top_supervisor.config.llm_config.base_url == "https://openrouter.ai/api/v1"
                
                sub_team = team.teams["openrouter_sub_team"]
                assert sub_team.supervisor.config.llm_config.provider == "openrouter"
                assert sub_team.agents[0].config.llm_config.provider == "openrouter"
                assert "llama" in sub_team.agents[0].config.llm_config.model
    
    @pytest.mark.asyncio
    async def test_aws_bedrock_provider_integration(self, mock_state_manager, mock_event_manager):
        """测试AWS Bedrock提供商集成"""
        from hierarchical_agents.env_key_manager import EnvironmentKeyManager
        
        # 创建AWS Bedrock密钥管理器
        key_manager = EnvironmentKeyManager()
        
        # Mock AWS环境变量
        aws_env = {
            'AWS_ACCESS_KEY_ID': 'AKIATEST123456789',
            'AWS_SECRET_ACCESS_KEY': 'test-secret-key-123456789',
            'AWS_DEFAULT_REGION': 'us-east-1'
        }
        
        with patch.dict(os.environ, aws_env):
            # 验证密钥读取
            credentials = key_manager.get_key("aws_bedrock")
            assert "AKIATEST123456789" in credentials
            assert "test-secret-key-123456789" in credentials
            
            # 验证密钥格式
            assert key_manager.validate_key_format("aws_bedrock", credentials)
            
            # Mock LLM客户端创建
            with patch.object(key_manager, 'get_llm_client') as mock_get_client:
                mock_llm = Mock()
                mock_llm.invoke = Mock(return_value=Mock(content="Bedrock响应"))
                mock_get_client.return_value = mock_llm
                
                # 创建AWS Bedrock配置
                bedrock_config = {
                    "team_name": "bedrock_team",
                    "description": "AWS Bedrock测试团队",
                    "top_supervisor_config": {
                        "llm_config": {
                            "provider": "aws_bedrock",
                            "model": "anthropic.claude-3-sonnet-20240229-v1:0",
                            "region": "us-east-1",
                            "temperature": 0.4,
                            "max_tokens": 4000
                        },
                        "system_prompt": "Bedrock监督者",
                        "user_prompt": "使用Bedrock协调团队",
                        "max_iterations": 10
                    },
                    "sub_teams": [{
                        "id": "bedrock_sub_team",
                        "name": "Bedrock子团队",
                        "description": "使用Bedrock的子团队",
                        "supervisor_config": {
                            "llm_config": {
                                "provider": "aws_bedrock",
                                "model": "anthropic.claude-3-haiku-20240307-v1:0",
                                "region": "us-west-2",
                                "temperature": 0.2
                            },
                            "system_prompt": "Bedrock子团队监督者",
                            "user_prompt": "使用Bedrock执行任务",
                            "max_iterations": 5
                        },
                        "agent_configs": [{
                            "agent_id": "bedrock_agent",
                            "agent_name": "Bedrock智能体",
                            "llm_config": {
                                "provider": "aws_bedrock",
                                "model": "amazon.titan-text-express-v1",
                                "region": "us-east-1",
                                "temperature": 0.6,
                                "max_tokens": 2500
                            },
                            "system_prompt": "Bedrock智能体",
                            "user_prompt": "使用Bedrock执行子任务",
                            "tools": [],
                            "max_iterations": 3
                        }]
                    }],
                    "dependencies": {},
                    "global_config": {
                        "max_execution_time": 3600,
                        "enable_streaming": True,
                        "output_format": "detailed"
                    }
                }
                
                # 构建团队
                manager = HierarchicalManager(key_manager=key_manager)
                team = manager.build_hierarchy(bedrock_config)
                
                # 验证AWS Bedrock配置
                assert team.top_supervisor.config.llm_config.provider == "aws_bedrock"
                assert "claude" in team.top_supervisor.config.llm_config.model
                assert team.top_supervisor.config.llm_config.region == "us-east-1"
                
                sub_team = team.teams["bedrock_sub_team"]
                assert sub_team.supervisor.config.llm_config.provider == "aws_bedrock"
                assert sub_team.supervisor.config.llm_config.region == "us-west-2"
                assert sub_team.agents[0].config.llm_config.provider == "aws_bedrock"
                assert "titan" in sub_team.agents[0].config.llm_config.model
    
    @pytest.mark.asyncio
    async def test_mixed_provider_team_execution(self, mock_state_manager, mock_event_manager):
        """测试混合提供商团队执行"""
        from hierarchical_agents.env_key_manager import EnvironmentKeyManager
        
        # 创建密钥管理器
        key_manager = EnvironmentKeyManager()
        
        # Mock所有提供商的环境变量
        mixed_env = {
            'OPENAI_API_KEY': 'sk-test-openai-key',
            'OPENROUTER_API_KEY': 'sk-or-test-openrouter-key',
            'AWS_ACCESS_KEY_ID': 'AKIATEST123456789',
            'AWS_SECRET_ACCESS_KEY': 'test-secret-key-123456789',
            'AWS_DEFAULT_REGION': 'us-east-1'
        }
        
        with patch.dict(os.environ, mixed_env):
            # Mock LLM客户端创建
            with patch.object(key_manager, 'get_llm_client') as mock_get_client:
                # 为不同提供商返回不同的Mock客户端
                def mock_client_factory(provider, model):
                    mock_llm = Mock()
                    mock_llm.invoke = Mock(return_value=Mock(content=f"{provider}响应"))
                    return mock_llm
                
                mock_get_client.side_effect = mock_client_factory
                
                # 创建混合提供商配置
                mixed_config = {
                    "team_name": "mixed_provider_team",
                    "description": "混合提供商测试团队",
                    "top_supervisor_config": {
                        "llm_config": {
                            "provider": "openai",
                            "model": "gpt-4o",
                            "temperature": 0.3
                        },
                        "system_prompt": "混合团队顶级监督者",
                        "user_prompt": "协调混合提供商团队",
                        "max_iterations": 10
                    },
                    "sub_teams": [
                        {
                            "id": "openai_team",
                            "name": "OpenAI团队",
                            "description": "使用OpenAI的团队",
                            "supervisor_config": {
                                "llm_config": {
                                    "provider": "openai",
                                    "model": "gpt-4o",
                                    "temperature": 0.5
                                },
                                "system_prompt": "OpenAI团队监督者",
                                "user_prompt": "使用OpenAI执行任务",
                                "max_iterations": 5
                            },
                            "agent_configs": [{
                                "agent_id": "openai_mixed_agent",
                                "agent_name": "OpenAI混合智能体",
                                "llm_config": {
                                    "provider": "openai",
                                    "model": "gpt-4o",
                                    "temperature": 0.7
                                },
                                "system_prompt": "OpenAI智能体",
                                "user_prompt": "执行OpenAI任务",
                                "tools": [],
                                "max_iterations": 3
                            }]
                        },
                        {
                            "id": "openrouter_team",
                            "name": "OpenRouter团队",
                            "description": "使用OpenRouter的团队",
                            "supervisor_config": {
                                "llm_config": {
                                    "provider": "openrouter",
                                    "model": "anthropic/claude-3-sonnet",
                                    "base_url": "https://openrouter.ai/api/v1",
                                    "temperature": 0.4
                                },
                                "system_prompt": "OpenRouter团队监督者",
                                "user_prompt": "使用OpenRouter执行任务",
                                "max_iterations": 5
                            },
                            "agent_configs": [{
                                "agent_id": "openrouter_mixed_agent",
                                "agent_name": "OpenRouter混合智能体",
                                "llm_config": {
                                    "provider": "openrouter",
                                    "model": "anthropic/claude-3-haiku",
                                    "base_url": "https://openrouter.ai/api/v1",
                                    "temperature": 0.6
                                },
                                "system_prompt": "OpenRouter智能体",
                                "user_prompt": "执行OpenRouter任务",
                                "tools": [],
                                "max_iterations": 3
                            }]
                        },
                        {
                            "id": "bedrock_team",
                            "name": "Bedrock团队",
                            "description": "使用Bedrock的团队",
                            "supervisor_config": {
                                "llm_config": {
                                    "provider": "aws_bedrock",
                                    "model": "anthropic.claude-3-sonnet-20240229-v1:0",
                                    "region": "us-east-1",
                                    "temperature": 0.3
                                },
                                "system_prompt": "Bedrock团队监督者",
                                "user_prompt": "使用Bedrock执行任务",
                                "max_iterations": 5
                            },
                            "agent_configs": [{
                                "agent_id": "bedrock_mixed_agent",
                                "agent_name": "Bedrock混合智能体",
                                "llm_config": {
                                    "provider": "aws_bedrock",
                                    "model": "amazon.titan-text-express-v1",
                                    "region": "us-east-1",
                                    "temperature": 0.5
                                },
                                "system_prompt": "Bedrock智能体",
                                "user_prompt": "执行Bedrock任务",
                                "tools": [],
                                "max_iterations": 3
                            }]
                        }
                    ],
                    "dependencies": {
                        "openrouter_team": ["openai_team"],
                        "bedrock_team": ["openrouter_team"]
                    },
                    "global_config": {
                        "max_execution_time": 3600,
                        "enable_streaming": True,
                        "output_format": "detailed"
                    }
                }
                
                # 构建混合团队
                manager = HierarchicalManager(key_manager=key_manager)
                team = manager.build_hierarchy(mixed_config)
                
                # 验证混合配置
                assert team.top_supervisor.config.llm_config.provider == "openai"
                assert team.teams["openai_team"].supervisor.config.llm_config.provider == "openai"
                assert team.teams["openrouter_team"].supervisor.config.llm_config.provider == "openrouter"
                assert team.teams["bedrock_team"].supervisor.config.llm_config.provider == "aws_bedrock"
                
                # 验证依赖关系
                assert team.execution_order is not None
                openai_index = team.execution_order.index("openai_team")
                openrouter_index = team.execution_order.index("openrouter_team")
                bedrock_index = team.execution_order.index("bedrock_team")
                
                assert openai_index < openrouter_index < bedrock_index
                
                # 创建执行器并测试执行
                executor = HierarchicalExecutor(
                    state_manager=mock_state_manager,
                    event_manager=mock_event_manager
                )
                
                # Mock混合执行
                async def mock_mixed_execution(team_obj, execution_id):
                    return {
                        "execution_id": execution_id,
                        "status": "completed",
                        "team_results": {
                            team_id: {
                                "status": "completed",
                                "provider": team_obj.teams[team_id].supervisor.config.llm_config.provider
                            }
                            for team_id in team_obj.teams.keys()
                        }
                    }
                
                with patch.object(executor, 'execute_hierarchical_team', side_effect=mock_mixed_execution):
                    result = await executor.execute_hierarchical_team(team, "mixed_exec_123")
                    
                    # 验证混合执行结果
                    assert result["status"] == "completed"
                    assert "openai_team" in result["team_results"]
                    assert "openrouter_team" in result["team_results"]
                    assert "bedrock_team" in result["team_results"]
                    
                    # 验证每个团队使用了正确的提供商
                    assert result["team_results"]["openai_team"]["provider"] == "openai"
                    assert result["team_results"]["openrouter_team"]["provider"] == "openrouter"
                    assert result["team_results"]["bedrock_team"]["provider"] == "aws_bedrock"
    
    @pytest.mark.asyncio
    async def test_provider_failover_mechanism(self, mock_state_manager, mock_event_manager):
        """测试提供商故障转移机制"""
        from hierarchical_agents.env_key_manager import EnvironmentKeyManager
        from hierarchical_agents.error_handler import ErrorHandler
        
        # 创建密钥管理器和错误处理器
        key_manager = EnvironmentKeyManager()
        error_handler = ErrorHandler()
        
        # Mock环境变量
        with patch.dict(os.environ, {
            'OPENAI_API_KEY': 'sk-test-openai-key',
            'OPENROUTER_API_KEY': 'sk-or-test-openrouter-key'
        }):
            # Mock LLM客户端，模拟OpenAI失败
            def mock_failing_client(provider, model):
                mock_llm = Mock()
                if provider == "openai":
                    mock_llm.invoke.side_effect = Exception("OpenAI API不可用")
                else:
                    mock_llm.invoke = Mock(return_value=Mock(content=f"{provider}响应"))
                return mock_llm
            
            with patch.object(key_manager, 'get_llm_client', side_effect=mock_failing_client):
                # 创建故障转移配置
                failover_config = {
                    "team_name": "failover_team",
                    "description": "故障转移测试团队",
                    "top_supervisor_config": {
                        "llm_config": {
                            "provider": "openai",
                            "model": "gpt-4o",
                            "temperature": 0.3,
                            "fallback_provider": "openrouter",
                            "fallback_model": "anthropic/claude-3-sonnet"
                        },
                        "system_prompt": "故障转移监督者",
                        "user_prompt": "协调故障转移团队",
                        "max_iterations": 10
                    },
                    "sub_teams": [{
                        "id": "failover_sub_team",
                        "name": "故障转移子团队",
                        "description": "测试故障转移",
                        "supervisor_config": {
                            "llm_config": {
                                "provider": "openai",
                                "model": "gpt-4o",
                                "temperature": 0.5,
                                "fallback_provider": "openrouter",
                                "fallback_model": "anthropic/claude-3-haiku"
                            },
                            "system_prompt": "故障转移子团队监督者",
                            "user_prompt": "执行故障转移任务",
                            "max_iterations": 5
                        },
                        "agent_configs": [{
                            "agent_id": "failover_agent",
                            "agent_name": "故障转移智能体",
                            "llm_config": {
                                "provider": "openai",
                                "model": "gpt-4o",
                                "temperature": 0.7,
                                "fallback_provider": "openrouter",
                                "fallback_model": "meta-llama/llama-3.1-8b-instruct"
                            },
                            "system_prompt": "故障转移智能体",
                            "user_prompt": "执行故障转移子任务",
                            "tools": [],
                            "max_iterations": 3
                        }]
                    }],
                    "dependencies": {},
                    "global_config": {
                        "max_execution_time": 3600,
                        "enable_streaming": True,
                        "output_format": "detailed"
                    }
                }
                
                # 构建团队
                manager = HierarchicalManager(key_manager=key_manager)
                team = manager.build_hierarchy(failover_config)
                
                # 创建执行器
                executor = HierarchicalExecutor(
                    state_manager=mock_state_manager,
                    event_manager=mock_event_manager,
                    error_handler=error_handler
                )
                
                # Mock故障转移执行
                async def mock_failover_execution(team_obj, execution_id):
                    # 模拟检测到OpenAI失败并转移到OpenRouter
                    return {
                        "execution_id": execution_id,
                        "status": "completed",
                        "failover_occurred": True,
                        "original_provider": "openai",
                        "fallback_provider": "openrouter",
                        "team_results": {
                            "failover_sub_team": {
                                "status": "completed",
                                "provider_used": "openrouter"
                            }
                        }
                    }
                
                with patch.object(executor, 'execute_hierarchical_team', side_effect=mock_failover_execution):
                    result = await executor.execute_hierarchical_team(team, "failover_exec_123")
                    
                    # 验证故障转移结果
                    assert result["status"] == "completed"
                    assert result["failover_occurred"] is True
                    assert result["original_provider"] == "openai"
                    assert result["fallback_provider"] == "openrouter"
                    assert result["team_results"]["failover_sub_team"]["provider_used"] == "openrouter"


# 运行测试的辅助函数
if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])